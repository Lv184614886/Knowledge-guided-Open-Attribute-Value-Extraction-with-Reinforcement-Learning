{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_preparation.ipynb","provenance":[{"file_id":"1B4Tini7F3Tv_nyS8CPjm7nFz7MBclEmk","timestamp":1602484297078}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"O04pkr6Z0j4G","outputId":"c6c1fa19-7cff-4c72-8fe9-75dad53281b4","colab":{"base_uri":"https://localhost:8080/","height":734}},"source":["#%%capture\n","print('NOTE: Intentionally crashing session to use the newly installed library.\\n')\n","!pip uninstall -y pyarrow\n","!pip install ray[debug]==0.7.5\n","# A hack to force the runtime to restart, needed to include the above dependencies.\n","import os\n","os._exit(0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NOTE: Intentionally crashing session to use the newly installed library.\n","\n","Uninstalling pyarrow-0.14.1:\n","  Successfully uninstalled pyarrow-0.14.1\n","Collecting ray[debug]==0.7.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/e7/37a7f8dc2b1f96c760a3950d8bb5a3f14066f1699f5d004f0f6462d880c9/ray-0.7.5-cp36-cp36m-manylinux1_x86_64.whl (74.9MB)\n","\u001b[K     |████████████████████████████████| 74.9MB 42kB/s \n","\u001b[?25hCollecting funcsigs\n","  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.13)\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.6.4)\n","Collecting redis>=3.3.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n","\u001b[K     |████████████████████████████████| 81kB 13.3MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.12.4)\n","Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (1.18.5)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (2.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (3.0.12)\n","Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (7.1.2)\n","Collecting py-spy; extra == \"debug\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/a7/ab45c9ee3c4654edda3efbd6b8e2fa4962226718a7e3e3be6e3926bf3617/py_spy-0.3.3-py2.py3-none-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 20.3MB/s \n","\u001b[?25hRequirement already satisfied: psutil; extra == \"debug\" in /usr/local/lib/python3.6/dist-packages (from ray[debug]==0.7.5) (5.4.8)\n","Collecting setproctitle; extra == \"debug\"\n","  Downloading https://files.pythonhosted.org/packages/5a/0d/dc0d2234aacba6cf1a729964383e3452c52096dc695581248b548786f2b3/setproctitle-1.1.10.tar.gz\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (1.9.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (1.4.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (8.5.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (50.3.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (20.2.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->ray[debug]==0.7.5) (0.7.1)\n","Building wheels for collected packages: setproctitle\n","  Building wheel for setproctitle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for setproctitle: filename=setproctitle-1.1.10-cp36-cp36m-linux_x86_64.whl size=33912 sha256=e92c3ca89ae2de96c1d14257e23cc240a83b559c747b8b8bba40d92125d26748\n","  Stored in directory: /root/.cache/pip/wheels/e6/b1/a6/9719530228e258eba904501fef99d5d85c80d52bd8f14438a3\n","Successfully built setproctitle\n","Installing collected packages: funcsigs, colorama, redis, py-spy, setproctitle, ray\n","Successfully installed colorama-0.4.4 funcsigs-1.0.2 py-spy-0.3.3 ray-0.7.5 redis-3.5.3 setproctitle-1.1.10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"68wgJkDehIXJ"},"source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzq9OclK0pg5"},"source":["%%capture\n","!pip install python-Levenshtein\n","!pip install redis\n","!pip install -U ray\n","!pip install ray[debug]==0.7.5\n","!pip install ray[rllib]  # also recommended: ray[debug]\n","!pip uninstall -y pyarrow\n","!pip install unicodedata2\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nSDz6m3g55L9"},"source":["## Import"]},{"cell_type":"code","metadata":{"id":"9vknvL5gqi3X"},"source":["import os\n","os.chdir(\"/content/drive/My Drive/Knowledge Extraction/szhang37_code/KG_RL\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LedW5b8H0mki"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H1vSh2nH0rST"},"source":["import tensorflow as tf\n","import pickle\n","import random\n","import pandas as pd \n","import numpy as np\n","import json\n","import time\n","\n","from collections import Counter\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import unicodedata\n","from functools import reduce\n","#### Import ray related package\n","import ray\n","from ray.rllib.models import ModelCatalog\n","from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n","from ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n","\n","from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n","from ray.rllib.models.tf.misc import normc_initializer\n","from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n","from ray.rllib.agents.dqn.distributional_q_model import DistributionalQModel\n","from ray.rllib.models.tf.visionnet_v2 import VisionNetwork as MyVisionNetwork\n","from ray.tune.logger import pretty_print\n","from ray.rllib.utils import try_import_tf\n","from ray.tune import grid_search\n","from ray.rllib.models import ModelCatalog\n","from ray.tune import Trainable\n","from ray.tune.logger import pretty_print\n","from ray.tune import run as run_tune\n","from ray.tune.registry import register_env\n","import gym\n","from gym import spaces\n","from gym.spaces import Discrete, Box\n","from ray import tune\n","from ray.rllib.agents.dqn.dqn import DQNTrainer, DEFAULT_CONFIG\n","### import self-defined function\n","import similarity_metrics\n","from utility_function import *\n","### import Environment\n","from Environment import KGRLEnv\n","### Import Customized DQN\n","from PolicyDQN import *\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V23fjcNy0Tpm"},"source":["## Data preparation\n","\n","This section is used to obtain predicted data.\n","\n","Skip this section when saved predicted data is provided.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z-hLZkZKsYI-"},"source":["### Bert\n","\n","distance training for bert"]},{"cell_type":"code","metadata":{"id":"-w_rxgeNrx32"},"source":["import os\n","import json\n","import pickle\n","########################################################\n","## convert the distance data into squad-like dataset ###\n","########################################################\n","\n","def convert_distance_datasquad(file_name_input = 'sougou.json', file_name_output = None):\n","  \n","  data = {}\n","  data['data'] = []\n","  f = open(file_name_input, \"r\")\n","  lines = f.readlines()\n","  for i in range(len(lines)):\n","    line = lines[i]\n","    oneline = json.loads(line)\n","    answer = oneline['answer']\n","    passages = oneline['passages']\n","    question = oneline['question']\n","    for j in range(len(passages)):\n","      start_id = passages[j]['passage_text'].find(answer)\n","      if start_id == -1: continue\n","      entry = {}\n","      entry[\"paragraphs\"] = [{\n","        'context' : passages[j]['passage_text'],\n","        'qas' : [{'question' : question,\n","                  'id' : \"Trainbaidu_\" + str(i) + \"_context_\" + str(j),\n","                  'answers' : [{'answer_start': start_id, \n","                                'text': answer}]  }]\n","      }]\n","      data['data'].append(entry)\n","      break \n","\n","  with open(file_name_output, 'w') as outfile:\n","      json.dump(data, outfile)\n","  return data\n","\n","distance_data_dir = \"./distance_data/\"\n","data = convert_distance_datasquad(file_name_input = distance_data_dir + 'sougou.json', file_name_output = distance_data_dir + 'sougou_squad.json')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gCBBV6QZt4qu"},"source":["#####################################\n","## covert our data into squad-like ##\n","#####################################\n","\n","def convert_squad(test_data, file_name = 'test_baidu_good.json'):\n","  \n","\n","  data = {}\n","  data['data'] = []\n","  for i in range(len(test_data)):\n","    answer = test_data[i]['o']\n","    passages = test_data[i]['corpus']\n","    question = test_data[i]['s'] + \"的\" + test_data[i]['p'] + \"是什么？\"\n","    for j in range(len(passages)):\n","      start_id = None\n","      entry = {}\n","      entry[\"paragraphs\"] = [{\n","        'context' : passages[j],\n","        'qas' : [{'question' : question,\n","                  'id' : \"Trainbaidu_\" + str(i) + \"_context_\" + str(j),\n","                  'answers' : [{'answer_start': start_id, \n","                                'text': answer}]  }]\n","      }]\n","      data['data'].append(entry)\n","      #print(i)\n","\n","        #break ## 每个问题有一个文本就好了。 (不然太太太多了？)\n","\n","\n","  with open(file_name, 'w') as outfile:\n","      json.dump(data, outfile)\n","\n","train_data = pickle.load(open(\"./preprocessed_data/train_data.pkl\", \"rb\" ))\n","test_data = pickle.load(open(\"./preprocessed_data/test_data.pkl\", \"rb\" ))\n","store_dir = \"./preprocessed_data/\"\n","convert_squad(test_data, file_name = store_dir + 'test_data_squad.json')\n","convert_squad(train_data, file_name = store_dir + 'train_data_squad.json')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VNvrREUFuPsI"},"source":["## ref https://github.com/google-research/bert\n","#####################################################\n","## use bert to fine tune and predict on our data   ##\n","#####################################################\n","\n","!python run_squad.py  \\\n","    --vocab_file={/chinese_L-12_H-768_A-12/vocab.txt} \\\n","    --bert_config_file={/chinese_L-12_H-768_A-12/bert_config.json} \\\n","    --init_checkpoint={/chinese_L-12_H-768_A-12/bert_model.ckpt} \\\n","    --do_train = True \\\n","    --train_file={path to sougou.json} \\\n","    --do_predict= False \\\n","    --predict_file= {path to our data (test_data_squad/train_data_squad.json)} \\\n","    --train_batch_size=12 --num_train_epochs=2.0 \\\n","    --max_seq_length=384 \\\n","    --doc_stride=128 \\\n","    --learning_rate=3e-5 \\\n","    --save_checkpoints_steps=1000 \\\n","    --output_dir=./output/distant_supervision_full \\\n","    --do_lower_case=True \\\n","    --use_tpu=False\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvH2CQEQxIDs"},"source":["############################\n","## pickle the bert output ##\n","############################\n","\n","def pickle_answer(input_data, pred_file = \"dev_predictions_test.json\",\\\n","                  pred_file_nbest = \"dev_nbest_predictions_test.json\",\\\n","                  file_name_output = \"pred_train_bert.pkl\", saving_path = \"./preprocessed_data/\"):\n","  \n","  \n","  with open(pred_file, \"r\") as read_file:\n","      dev_data = json.load(read_file)\n","\n","  with open(pred_file_nbest, \"r\") as read_file:\n","      dev_data_nbest = json.load(read_file)\n","\n","  pred_output = {}\n","  for i in range(len(input_data)):\n","    tmp = []\n","    for j in range(len(input_data[i]['corpus'])):\n","      dev_id = 'Trainbaidu_%d_context_%d' %(i, j)\n","      try:\n","        tmp.append( [ [dev_data[dev_id]], [dev_data_nbest[dev_id][0]['probability']] ])\n","      except:\n","        tmp.append( [ [\"ERROR\"], [0] ])\n","    pred_output[input_data[i]['id']] = tmp\n","  with open(saving_path + file_name_output, 'wb') as f:\n","    pickle.dump(pred_output, f)\n","  \n","saving_path = \"./preprocessed_data/\"\n","bert_output_path = \"./bert_output/\"\n","\n","train_data = pickle.load(open(\"./preprocessed_data/train_data.pkl\", \"rb\" ))\n","test_data = pickle.load(open(\"./preprocessed_data/test_data.pkl\", \"rb\" ))\n","\n","pickle_answer(input_data = test_data, pred_file = bert_output_path + \"dev_predictions_test.json\",\\\n","              pred_file_nbest = bert_output_path + \"dev_nbest_predictions_test.json\",\\\n","              file_name_output = \"pred_test_bert.pkl\", saving_path = saving_path)\n","\n","pickle_answer(input_data = train_data, pred_file = bert_output_path + \"dev_predictions_train.json\",\\\n","              pred_file_nbest = bert_output_path + \"dev_nbest_predictions_train.json\",\\\n","              file_name_output = \"pred_train_bert.pkl\", saving_path = saving_path)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bMr98X1VsdWA"},"source":["### QANet"]},{"cell_type":"code","metadata":{"id":"QKs8AN9VseoO","executionInfo":{"status":"ok","timestamp":1602967424110,"user_tz":240,"elapsed":32395,"user":{"displayName":"Sheng Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhraDPO3yxolOf82I55EVZaIAfn4g1oUU6mSS7k=s64","userId":"16336041814041002028"}},"outputId":"64dd993b-ca49-4d31-f73c-084c25b92662","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# load pre-trained QANet\n","os.chdir(\"./qanet_model\")\n","import qanet as net\n","from databunch import *\n","LoadModel(net)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["qanet_mixall_alter0.h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mvWQK3nbxWpg"},"source":["def find_s_list(data, train = True):\n","  if train:\n","    s_list = set()\n","    for i in range(len(data)):\n","      s_list.add(data[i]['s'])\n","    s_list = list(s_list)\n","  else:\n","    s_list = set()\n","    for i in range(len(data)):\n","      s_list.add(data[i][0])\n","    s_list = list(s_list)\n","  return s_list\n","\n","def GetAnswerListNew2(db_test, y_pred_start, y_pred_end, listans=True, addscore=[]):\n","    def GetAnswer(i, start, end):\n","        return ''.join(db_test.contextRaw[i][start:end + 1])\n","    answer = []\n","    predAnswerList = []\n","    for ii in range(db_test.numQuestions):\n","        cs, cspos = {}, {}\n","        for i in range(db_test.startEnd[ii][0], db_test.startEnd[ii][1] + 1):\n","            thisMax = [-1e+5] ;\n","            canswer = [\"NaN**\"] \n","            for j1 in range(maxPLen):\n","                for j2 in range(j1, min(maxPLen, j1 + 8)):\n","                    score = y_pred_start[i][j1] * y_pred_end[i][j2]\n","                    if score > min(thisMax) or canswer == \"NaN**\":\n","                        temp = GetAnswer(i, j1, j2)\n","                        if temp in db_test.questionRaw[ii]: continue\n","                        canswer[np.argmin(thisMax)] = temp\n","                        thisMax[np.argmin(thisMax)] = score                        \n","                        mj1, mj2 = j1, j2\n","            # one passage\n","            if listans:\n","                tlen = len(db_test.contextRaw[i])\n","                if mj2 + 1 < tlen and db_test.contextRaw[i][mj2 + 1] == '、':\n","                    jj2 = mj2 + 1\n","                    while jj2 < tlen:\n","                        token = db_test.contextRaw[i][jj2];\n","                        jj2 += 1\n","                        #print(token)\n","                        if token[0] in '，。,. 被是': break\n","                        canswer += token;\n","                        mj2 += 1\n","            answer.append([canswer, thisMax])\n","    return answer\n","\n","def GetCandidateAnswer(zz, query):\n","    ret = {'answer': '@NULL@', 'query': query, 'query_id': '0'}\n","    passages = []\n","    for text in zz:\n","        z = {'url': '', 'passage_text': text}\n","        passages.append(z)\n","    ret['passages'] = passages\n","    db = DataBunch(None, False, onejson=ret)\n","    X, Y = db.GetData()\n","    y_pred_start, y_pred_end = net.mm.predict(X, batch_size=128)  # ;print(y_pred_start,y_pred_end)\n","    corpus_addscore = [0.5, 0.5]\n","    isanslist = False\n","    ret = GetAnswerListNew2(db, y_pred_start, y_pred_end, isanslist, addscore=corpus_addscore)\n","    # print(ret)\n","    return ret\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMdpY5h8xblU"},"source":["os.chdir(\"/content/drive/My Drive/Knowledge Extraction/szhang37_code/KG_RL\")\n","train_data = pickle.load(open(\"./preprocessed_data/train_data.pkl\", \"rb\" ))\n","test_data = pickle.load(open(\"./preprocessed_data/test_data.pkl\", \"rb\" ))\n","test_data_types = [test_data[i]['type'] for i in range(len(test_data))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K4_542ul0Rtn","executionInfo":{"status":"ok","timestamp":1602967463261,"user_tz":240,"elapsed":39102,"user":{"displayName":"Sheng Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhraDPO3yxolOf82I55EVZaIAfn4g1oUU6mSS7k=s64","userId":"16336041814041002028"}},"outputId":"b9a83a0b-d250-4be6-e313-9b6ae5a5246a","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["train_pred = {}\n","for i in range(len(train_data)):\n","  key = 'Train_'+ str(i)\n","  entry = train_data[i]\n","  text_list = entry['corpus']\n","  query = entry['s'] + ' ' + entry['p']\n","  answer_list = GetCandidateAnswer(text_list, query)\n","  train_pred[key] = answer_list"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Building prefix dict from the default dictionary ...\n","Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 0.741 seconds.\n","Prefix dict has been built successfully.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"10cYqm6hysZG"},"source":["test_pred = {}\n","for i in range(len(test_data)):\n","  key = 'Test_'+ str(i)\n","  entry = test_data[i]\n","  text_list = entry['corpus']\n","  query = entry['s'] + ' ' + entry['p']\n","  answer_list = GetCandidateAnswer(text_list, query)\n","  test_pred[key] = answer_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfhLw-3q0Zqt"},"source":["saving_path = \"./preprocessed_data/\"\n","with open(saving_path+\"pred_train_qanet.pkl\", 'wb') as f:\n","  pickle.dump(train_pred, f)\n","with open(saving_path+\"pred_test_qanet.pkl\", 'wb') as f:\n","  pickle.dump(test_pred, f)"],"execution_count":null,"outputs":[]}]}